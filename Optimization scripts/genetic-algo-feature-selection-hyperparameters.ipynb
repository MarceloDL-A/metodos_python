{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03228,
     "end_time": "2021-01-11T11:58:14.548293",
     "exception": false,
     "start_time": "2021-01-11T11:58:14.516013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Genetic Algorithms: Feature Selection & Hyperparameters\n",
    "\n",
    "![](https://g-scop.grenoble-inp.fr/medias/photo/gscop-rub-oc_1425923414759-png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03043,
     "end_time": "2021-01-11T11:58:14.609878",
     "exception": false,
     "start_time": "2021-01-11T11:58:14.579448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook aims to combine my love of genetic algorithms with the model building steps of feature selection and hyperparameter optimisation. I have previously used genetic algorithms to choose which features to include in a model and then as a secondary process conducted hyperparameter optimisation. This time I want to see if the two steps can be integrated.\n",
    "\n",
    "This notebook has 2 primary objectives:\n",
    "1. Serve as practice using Python as I am typically an R user\n",
    "1. Apply Genetic Algorithms to combined feature selection and hyperparameter optimisation\n",
    "\n",
    "I believe there are two main ways to achieve the optimisation of feature selection and hyperparameters:\n",
    "\n",
    "1. Nested: For a candidate solution, firstly choose which features to include and then optimise hyperparameters\n",
    "1. Simualtaneous: For a candidate solution choose both feature subsets and hyperparameters at the same time\n",
    "\n",
    "This notebook explores the first case, the reason for this is it is thought that nesting will provide a greater range of hyperparameter values for each model. As a result this likely means that this approach will take longer than the simualtaneous approach.\n",
    "\n",
    "The 'Santander Customer Satisfaction' dataset will be used within this notebook as it high dimensional (300+ predictors), providing a good opportunity to test the optimisation process (especially the third objective). \n",
    "\n",
    "The notebook has been split into the following sections:\n",
    "\n",
    "* [Introducing Genetic Algorithms](#section-one)\n",
    "* [Prepare Environment](#section-two)\n",
    "* [Basic Data Insight & Health Check](#section-three)\n",
    "* [Validation & Cross Validation](#section-four)\n",
    "* [Preprocessing](#section-five)\n",
    "* [Optimisation Model Development](#section-six)\n",
    "    - [Generate Population](#section-six-subsection-one)\n",
    "    - [Solution Evaluation](#section-six-subsection-two)   \n",
    "* [Model Optimisation](#section-seven)\n",
    "    - [ElasticNet](#section-seven-subsection-one)\n",
    "    - [XGBOOST](#section-seven-subsection-two)\n",
    "    - [Analysis Findings](#section-seven-subsection-three)      \n",
    "* [Model Evaluation](#section-eight)\n",
    "* [Conclusion](#section-nine)  \n",
    "* [Submission](#section-ten)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030411,
     "end_time": "2021-01-11T11:58:14.671424",
     "exception": false,
     "start_time": "2021-01-11T11:58:14.641013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-one\"></a>\n",
    "## Introducing Genetic Algorithms\n",
    "\n",
    "Genetic Algorithms (GA) are a stochastic optimisation tool which draws inspiration from biological evolution. GA aims to minimise (or maximise in this case) a fitness function by changing decision variables. The core concepts of genetic algorithms are:\n",
    "\n",
    "* Fitness function - This is the function or measure which we want to maximise or minimise\n",
    "* Gene - An individual part of a solution\n",
    "* Chromosome - The combined gene's to create a single solution\n",
    "* Population - This is a predifined number of different Chromosomes to have a family of solutions\n",
    "* Generation - The model will run for a predifined number of iterations/ generations. Within each generation a new 'population' will be created and evaluated\n",
    "* Selection - The core concept of GA is survival of the fittest, the selection process allows us to probabilistically choose the best solutions in the current generation to be progressed for the next generation\n",
    "* Crossover - Once the fittest solutions are selected, crossover is the process of merging two solutions with the aim of producing offspring which has a better fitness score \n",
    "* Mutation - This is the process of introducing randomness into the population by probabilistically changing elements of a solution\n",
    "* Elitism - This is ensuring that the best solution in the current generation enters the next generation without being altered at all\n",
    "\n",
    "The image below provides a visual guide to how the optimisation process works.\n",
    "\n",
    "![](https://www.neuraldesigner.com/images/genetic_algorithm.png)\n",
    "\n",
    "\n",
    "### Relating GA to Feature Selection and Hyperparameters\n",
    "\n",
    "In the context of feature selection and hyperparameter optimisation, some elements of GA are described below:\n",
    "\n",
    "* Fitness function - An evaluation metric such AUC, with higher values being considered as more fit solutions\n",
    "* Gene - An individual predictor encoded as a binary flag (1 = included, 0 = excluded). It can also be an individual hyperparameter taking any real value\n",
    "* Chromosome - All of the individual features which were flagged as included, plus the hyperparameter values choosen\n",
    "* Population - A predefined number of models which have different features (included/ excluded) and hyperparameters of varying values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030311,
     "end_time": "2021-01-11T11:58:14.733034",
     "exception": false,
     "start_time": "2021-01-11T11:58:14.702723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-two\"></a>\n",
    "## Prepare Environment\n",
    "\n",
    "Now that we have a basic understanding of GA, we prepare the environment to build the optimisation model and search for a good predictive model (features and hyperparameters) for this dataset. In this section we prepare the environment for analysis which includes loading libraries for functionality and data to work with. \n",
    "\n",
    "The following packages are used in this analysis:\n",
    "\n",
    "1. Numpy: Numerical computing\n",
    "1. Pandas: Dataframes\n",
    "1. Scipy: Similarity measures\n",
    "1. Sklearn: Supervised Learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install sklearn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 8.54959,
     "end_time": "2021-01-11T11:58:23.313142",
     "exception": false,
     "start_time": "2021-01-11T11:58:14.763552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "# -------------------------------------------------- Libraries --------------------------------------------------\n",
    "#################################################################################################################\n",
    "\n",
    "# Numerical computing & dataframes\n",
    "import numpy as np # numerical computing\n",
    "import pandas as pd # dataframes\n",
    "import numpy.random as rnd # Pseudo random numbers\n",
    "from scipy import spatial\n",
    "\n",
    "# Modelling with sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import metrics\n",
    "\n",
    "# Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# warnings\n",
    "# from sklearn_utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Visuals\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "\n",
    "#################################################################################################################\n",
    "# -------------------------------------------------- Load Data --------------------------------------------------\n",
    "############################################################################################################\"#####\n",
    "\n",
    "# Load Data\n",
    "df_train = pd.read_csv(\"family_data.csv\")\n",
    "df_test = pd.read_csv(\"family_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030883,
     "end_time": "2021-01-11T11:58:23.375796",
     "exception": false,
     "start_time": "2021-01-11T11:58:23.344913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-three\"></a>\n",
    "## Basic Data Insight & Health Check\n",
    "\n",
    "The output below shows that we have 76K* rows in this dataset and 370 columns (quite high dimensional). The number of columns was one of the reasons why this dataset was choosen. Our target variable is binary and we can see that it is highly unbalanced with 96% of observations having the 0 trait (dissatisfied) and only 4% having the 1 trait (satisfied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['family_id', 'choice_0', 'choice_1', 'choice_2', 'choice_3', 'choice_4',\n",
       "       'choice_5', 'choice_6', 'choice_7', 'choice_8', 'choice_9', 'n_people'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:23.446315Z",
     "iopub.status.busy": "2021-01-11T11:58:23.445605Z",
     "iopub.status.idle": "2021-01-11T11:58:23.459222Z",
     "shell.execute_reply": "2021-01-11T11:58:23.458640Z"
    },
    "papermill": {
     "duration": 0.052333,
     "end_time": "2021-01-11T11:58:23.459334",
     "exception": false,
     "start_time": "2021-01-11T11:58:23.407001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 rows and 12 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4    0.2902\n",
       "3    0.1962\n",
       "5    0.1798\n",
       "2    0.1434\n",
       "6    0.0988\n",
       "7    0.0602\n",
       "8    0.0314\n",
       "Name: n_people, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################################################\n",
    "# ------------------------------------------------- Data Insight ------------------------------------------------\n",
    "#################################################################################################################\n",
    "\n",
    "# Dataset shape\n",
    "print(\"{} rows and {} columns\".format(*df_train.shape))\n",
    "\n",
    "# Target variable\n",
    "# df_train.TARGET.value_counts(normalize=True)\n",
    "df_train.n_people.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032337,
     "end_time": "2021-01-11T11:58:23.523726",
     "exception": false,
     "start_time": "2021-01-11T11:58:23.491389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The outputs below provide some intuition around the dataset, we can see that all predictors are numeric fields, with 259 being integer and the remaining 111 being of type float.\n",
    "\n",
    "Running a quick check for missingness, we see that there are no cells with 'nan' but when looking at the summary stats of 'var3' we can see that some values are coded as -999999 which indicates a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:23.597642Z",
     "iopub.status.busy": "2021-01-11T11:58:23.596947Z",
     "iopub.status.idle": "2021-01-11T11:58:23.687045Z",
     "shell.execute_reply": "2021-01-11T11:58:23.686423Z"
    },
    "papermill": {
     "duration": 0.131307,
     "end_time": "2021-01-11T11:58:23.687169",
     "exception": false,
     "start_time": "2021-01-11T11:58:23.555862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int64    12\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Missing entries: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    5000.000000\n",
       "mean       40.715000\n",
       "std        29.054208\n",
       "min         1.000000\n",
       "25%        16.000000\n",
       "50%        38.000000\n",
       "75%        62.000000\n",
       "max       100.000000\n",
       "Name: choice_0, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################################################################################################################\n",
    "# ------------------------------------------------- Data Health -------------------------------------------------\n",
    "#################################################################################################################\n",
    "\n",
    "# Data types\n",
    "display(df_train.dtypes.value_counts())\n",
    "\n",
    "# No missing observations in the dataset.\n",
    "print(\"Number of Missing entries: \" + str(df_train.isnull().sum().sum()))\n",
    "\n",
    "# Describe date\n",
    "df_train.choice_0.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033214,
     "end_time": "2021-01-11T11:58:23.758461",
     "exception": false,
     "start_time": "2021-01-11T11:58:23.725247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The output below shows the percentage of rows which the missing value represents in 'var3'. As the missing entries relate to a very small proportion we can replace it with the median value which was shown to be two above, which leaves us with a nice health dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:23.839896Z",
     "iopub.status.busy": "2021-01-11T11:58:23.838851Z",
     "iopub.status.idle": "2021-01-11T11:58:23.848968Z",
     "shell.execute_reply": "2021-01-11T11:58:23.848282Z"
    },
    "papermill": {
     "duration": 0.053684,
     "end_time": "2021-01-11T11:58:23.849083",
     "exception": false,
     "start_time": "2021-01-11T11:58:23.795399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     0.0744\n",
       "17    0.0212\n",
       "12    0.0200\n",
       "Name: choice_0, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View proprtion of missingness\n",
    "display(df_train['choice_0'].value_counts(normalize=True)[0:3])\n",
    "\n",
    "# Replace missing value\n",
    "df_train['choice_0'] = df_train['choice_0'].replace(-999999, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035006,
     "end_time": "2021-01-11T11:58:23.918629",
     "exception": false,
     "start_time": "2021-01-11T11:58:23.883623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-four\"></a>\n",
    "## Validation & Cross Validation\n",
    "\n",
    "As this dataset was part of a kaggle competition there is a specific test dataset. To support in the model building process a validation set will be created, which will account for 80% of the training data (reducing the actual training data to 80%). This will enable us to evaluate potential solutions quickly.\n",
    "\n",
    "5 fold cross validation will be applied to the new training data within the optimisation process to help get a stable estimate of which features and hyperparameters to include as our final models. \n",
    "\n",
    "Once we are happy with our model choice we will build the model on the full training data (train + validation) and use it to predict the test data provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:23.994609Z",
     "iopub.status.busy": "2021-01-11T11:58:23.993542Z",
     "iopub.status.idle": "2021-01-11T11:58:24.271044Z",
     "shell.execute_reply": "2021-01-11T11:58:24.270414Z"
    },
    "papermill": {
     "duration": 0.318386,
     "end_time": "2021-01-11T11:58:24.271162",
     "exception": false,
     "start_time": "2021-01-11T11:58:23.952776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "# -------------------------------------------------- Split Data -------------------------------------------------\n",
    "#################################################################################################################\n",
    "\n",
    "# Create a validation and training dataset\n",
    "df_train, df_validation = train_test_split(df_train,\n",
    "                                           test_size=0.2,\n",
    "                                           random_state=1989,\n",
    "                                           stratify=df_train.n_people,\n",
    "                                           shuffle=True)\n",
    "\n",
    "#################################################################################################################\n",
    "# ----------------------------------------------- Cross Validation ----------------------------------------------\n",
    "#################################################################################################################\n",
    "\n",
    "# Store the Kfold object\n",
    "kfold = KFold(n_splits=5, random_state=1989, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034416,
     "end_time": "2021-01-11T11:58:24.339950",
     "exception": false,
     "start_time": "2021-01-11T11:58:24.305534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-five\"></a>\n",
    "## Preprocessing\n",
    "\n",
    "In this section we will apply a simple standardisation and range scaler to all of our numeric variables. In general it is vital to conduct an in-depth exploratory analysis as well as feature engineering, however as this is not the purpose of this notebook, it will be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:24.616709Z",
     "iopub.status.busy": "2021-01-11T11:58:24.478605Z",
     "iopub.status.idle": "2021-01-11T11:58:27.106628Z",
     "shell.execute_reply": "2021-01-11T11:58:27.107110Z"
    },
    "papermill": {
     "duration": 2.73241,
     "end_time": "2021-01-11T11:58:27.107272",
     "exception": false,
     "start_time": "2021-01-11T11:58:24.374862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['TARGET'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\Estudos-M\\metodos_python\\metodos_python\\Optimization scripts\\genetic-algo-feature-selection-hyperparameters.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/genetic-algo-feature-selection-hyperparameters.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39m#################################################################################################################\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/genetic-algo-feature-selection-hyperparameters.ipynb#ch0000016?line=1'>2</a>\u001b[0m \u001b[39m# ----------------------------------------------- Preprocess Data -----------------------------------------------\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/genetic-algo-feature-selection-hyperparameters.ipynb#ch0000016?line=2'>3</a>\u001b[0m \u001b[39m#################################################################################################################\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/genetic-algo-feature-selection-hyperparameters.ipynb#ch0000016?line=3'>4</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/genetic-algo-feature-selection-hyperparameters.ipynb#ch0000016?line=4'>5</a>\u001b[0m \u001b[39m# Identify columns\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/genetic-algo-feature-selection-hyperparameters.ipynb#ch0000016?line=5'>6</a>\u001b[0m fts_num \u001b[39m=\u001b[39m df_train\u001b[39m.\u001b[39;49mdrop(axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mTARGET\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mselect_dtypes(np\u001b[39m.\u001b[39mnumber)\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/genetic-algo-feature-selection-hyperparameters.ipynb#ch0000016?line=7'>8</a>\u001b[0m \u001b[39m# Numerical Transformer StandardScaler\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/genetic-algo-feature-selection-hyperparameters.ipynb#ch0000016?line=8'>9</a>\u001b[0m trans_num \u001b[39m=\u001b[39m Pipeline(steps \u001b[39m=\u001b[39m [(\u001b[39m'\u001b[39m\u001b[39mStandarise\u001b[39m\u001b[39m'\u001b[39m, StandardScaler()), (\u001b[39m'\u001b[39m\u001b[39mMinMax\u001b[39m\u001b[39m'\u001b[39m, MinMaxScaler())])\n",
      "File \u001b[1;32md:\\Estudos-M\\metodos_python\\metodos_python\\Optimization scripts\\env\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Estudos-M\\metodos_python\\metodos_python\\Optimization scripts\\env\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4805'>4806</a>\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4806'>4807</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4807'>4808</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4814'>4815</a>\u001b[0m     errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4815'>4816</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4816'>4817</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4817'>4818</a>\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4818'>4819</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4951'>4952</a>\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4952'>4953</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4953'>4954</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4954'>4955</a>\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4955'>4956</a>\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4956'>4957</a>\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4957'>4958</a>\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4958'>4959</a>\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4959'>4960</a>\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4960'>4961</a>\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/frame.py?line=4961'>4962</a>\u001b[0m     )\n",
      "File \u001b[1;32md:\\Estudos-M\\metodos_python\\metodos_python\\Optimization scripts\\env\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4264'>4265</a>\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4265'>4266</a>\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4266'>4267</a>\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4268'>4269</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4269'>4270</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32md:\\Estudos-M\\metodos_python\\metodos_python\\Optimization scripts\\env\\lib\\site-packages\\pandas\\core\\generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4308'>4309</a>\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4309'>4310</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4310'>4311</a>\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4311'>4312</a>\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4313'>4314</a>\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/generic.py?line=4314'>4315</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Estudos-M\\metodos_python\\metodos_python\\Optimization scripts\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/indexes/base.py?line=6641'>6642</a>\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/indexes/base.py?line=6642'>6643</a>\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/indexes/base.py?line=6643'>6644</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/indexes/base.py?line=6644'>6645</a>\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   <a href='file:///d%3A/Estudos-M/metodos_python/metodos_python/Optimization%20scripts/env/lib/site-packages/pandas/core/indexes/base.py?line=6645'>6646</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['TARGET'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#################################################################################################################\n",
    "# ----------------------------------------------- Preprocess Data -----------------------------------------------\n",
    "#################################################################################################################\n",
    "\n",
    "# Identify columns\n",
    "fts_num = df_train.drop(axis=1,columns=['TARGET']).select_dtypes(np.number).columns\n",
    "\n",
    "# Numerical Transformer StandardScaler\n",
    "trans_num = Pipeline(steps = [('Standarise', StandardScaler()), ('MinMax', MinMaxScaler())])\n",
    "\n",
    "# Create a single Preprocessing step for predictors\n",
    "preprocessor_preds = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', trans_num, fts_num) # Centre and scale and constrain range\n",
    "    ])\n",
    "\n",
    "# Apply the transformations to train\n",
    "df_train2 = pd.DataFrame(preprocessor_preds.fit_transform(df_train))\n",
    "df_train2.columns = fts_num\n",
    "\n",
    "# Apply the transformations to validation\n",
    "df_validation2 = pd.DataFrame(preprocessor_preds.fit_transform(df_validation))\n",
    "df_validation2.columns = fts_num\n",
    "\n",
    "# Apply the transformations to test\n",
    "df_test2 = pd.DataFrame(preprocessor_preds.fit_transform(df_test))\n",
    "df_test2.columns = fts_num\n",
    "\n",
    "# Create preprocessed training data\n",
    "df_train = pd.concat([df_train2,\n",
    "                      df_train.drop(axis=1,columns=fts_num).reset_index().drop(axis=1,columns=['ID'])],\n",
    "                     axis=1)\n",
    "\n",
    "# Create preprocessed validation data\n",
    "df_validation = pd.concat([df_validation2,\n",
    "                           df_validation.drop(axis=1,columns=fts_num).reset_index().drop(axis=1,columns=['ID'])],\n",
    "                          axis=1)\n",
    "\n",
    "# Create preprocessed test data\n",
    "df_test = pd.concat([df_test2,\n",
    "                     df_test.drop(axis=1,columns=fts_num).reset_index().drop(axis=1,columns=['ID'])],\n",
    "                    axis=1)\n",
    "\n",
    "# Clear objects\n",
    "del df_train2, df_validation2, df_test2, fts_num, trans_num, preprocessor_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034319,
     "end_time": "2021-01-11T11:58:27.175996",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.141677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-six\"></a>\n",
    "## Optimisation Model Development\n",
    "\n",
    "In this section we walk through the functions which will make up our GA optimisation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034659,
     "end_time": "2021-01-11T11:58:27.244964",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.210305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-six-subsection-one\"></a>\n",
    "### Generate Population\n",
    "\n",
    "This section focusses on generating a population of solutions adhering to the principles with GA for each generation. \n",
    "\n",
    "The first function below generates the initial solutions for both features and hyperparameters.\n",
    "\n",
    "Choosing Initial Features:\n",
    "1. For each solution in the population randomly choose a percentage between 10-91% to be applied to the total number of features\n",
    "1. For each solution randomly choose features until it accounts for the percentage of columns of the total columns\n",
    "\n",
    "Choosing Initial Hyperparameters:\n",
    "1. For each solution generate a random value for each hyperparameter between a user stated min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:27.332592Z",
     "iopub.status.busy": "2021-01-11T11:58:27.327390Z",
     "iopub.status.idle": "2021-01-11T11:58:27.335282Z",
     "shell.execute_reply": "2021-01-11T11:58:27.335784Z"
    },
    "papermill": {
     "duration": 0.056828,
     "end_time": "2021-01-11T11:58:27.335969",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.279141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Randomly generate candidates\n",
    "def f_random_candidates(features_name, population, hyperparams, output_type, df_pop=False):\n",
    "    '''create an initial population'''\n",
    "   \n",
    "    # Create solution for features\n",
    "    if output_type == 'feature':\n",
    "        \n",
    "        # Initial population will have between 10-91% of features\n",
    "        feature_size = rnd.choice(a=range(10,91),size=population, replace=True)\n",
    "        feature_size = [np.round(pct / 100 * len(features_name)) for pct in feature_size]\n",
    "        \n",
    "        # Create a list of feature positions for each candidate\n",
    "        selection = [rnd.choice(a=range(0,len(features_name)-1), replace=False, size=cols.astype('int')) \\\n",
    "                     for cols in feature_size]\n",
    "        \n",
    "        selection = [list(selection[i]) for i in range(len(selection))]\n",
    "        \n",
    "        # Return\n",
    "        return selection\n",
    "    \n",
    "    # Create solution for hyperparameters\n",
    "    elif (output_type == 'hyperparams') & (hyperparams != False):\n",
    "        \n",
    "        # Generate random numbers in range for each hyperparameter\n",
    "        random_hyperparams = []\n",
    "        for j in range(len(hyperparams['names'])):\n",
    "            temp = (np.random.uniform(hyperparams['min_value'][j],\n",
    "                                      hyperparams['max_value'][j],\n",
    "                                      population))\n",
    "            random_hyperparams.append(temp)\n",
    "        \n",
    "        # Get length of features\n",
    "        n_features = df_pop['features'].apply(len).tolist()\n",
    "\n",
    "        # Store hyperparameters in diction\n",
    "        hyperparam_vals = []\n",
    "        for i in range(population):\n",
    "            val = {'name':[],'value':[]}\n",
    "            for j in range(len(hyperparams['names'])):\n",
    "                val['name'].append(hyperparams['names'][j])\n",
    "                temp = random_hyperparams[j][i]\n",
    "                if hyperparams['type'][j] == 'int':\n",
    "                    temp = np.int64(round(temp))\n",
    "                if hyperparams['names'][j] == 'max_features':\n",
    "                    temp = min(temp, n_features[i])\n",
    "                val['value'].append(temp)           \n",
    "            \n",
    "            hyperparam_vals.append(val)\n",
    "            del val\n",
    "\n",
    "        # Return\n",
    "        return hyperparam_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034538,
     "end_time": "2021-01-11T11:58:27.404842",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.370304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From the second generation onwards, we can use the crossover function to generate new solutions for the next generation. This function allows us to merge good solutions with the aim of creating a better solution, this is done for both hyperparameters and feature selection.\n",
    "\n",
    "Crossover Features:\n",
    "1. Randomly generate an integer which represents a cross point between the first and last feature (ultimately a column index)\n",
    "1. Weighted sampling of the previous generation's solutions and select two parents\n",
    "1. Create a child solution which have all the features before the cross point (column index) from first parent and all the features from the second parent after the cross point\n",
    "\n",
    "Crossover Hyperparameters:\n",
    "1. Weighted sample of previous generation solutions of the size of the number of hyperparameters\n",
    "1. Randomly choose which hyperparameter to take from which parent solution\n",
    "1. Create the child hyperparameters from the choosen parent solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:27.490080Z",
     "iopub.status.busy": "2021-01-11T11:58:27.484358Z",
     "iopub.status.idle": "2021-01-11T11:58:27.493179Z",
     "shell.execute_reply": "2021-01-11T11:58:27.492644Z"
    },
    "papermill": {
     "duration": 0.053771,
     "end_time": "2021-01-11T11:58:27.493297",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.439526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crossover function\n",
    "def f_gen_child_crossover(df, features_name, hyperparams, output_type):\n",
    "    '''Mutate 2 parents to create a child'''\n",
    "    \n",
    "    # Crossover features\n",
    "    if output_type == 'feature':\n",
    "        \n",
    "        # Create an integer list of features\n",
    "        l_features = list(range(0,len(features_name)))\n",
    "           \n",
    "        # Identify a random cross over point\n",
    "        cross_point = np.int(rnd.randint(low=0, high=len(features_name), size=1))\n",
    "        \n",
    "        # Extract Two Parents\n",
    "        selection = np.random.choice(df.features, \n",
    "                                     size=2, \n",
    "                                     replace=False, \n",
    "                                     p=df.probability)     \n",
    "        par1 = list(selection[0])\n",
    "        par2 = list(selection[1])\n",
    "            \n",
    "        # Convert to Boolean\n",
    "        par1 = [item in par1 for item in l_features]\n",
    "        par2 = [item in par2 for item in l_features]\n",
    "        \n",
    "        # Single point cross over and convert to indices\n",
    "        child = par1[0:cross_point] + par2[cross_point:]\n",
    "        child = [i for i,x in enumerate(child) if x == True]    \n",
    "    \n",
    "        # Return \n",
    "        return child\n",
    "    \n",
    "    # Crossover hyperparameters\n",
    "    elif (output_type == 'hyperparams') & (hyperparams != False):\n",
    "        \n",
    "        # Identify the number of parameters\n",
    "        n_hyperparameters = len(hyperparams['min_value'])\n",
    "        \n",
    "        # Extract n Parents\n",
    "        selection = np.random.choice(df.hyperparameters, \n",
    "                                     size=n_hyperparameters, \n",
    "                                     replace=False, \n",
    "                                     p=df.probability)  \n",
    "        \n",
    "        # Randomly choose which parent to select each parameter from\n",
    "        parent_choice = list(np.random.choice(range(n_hyperparameters),\n",
    "                                              size = n_hyperparameters,\n",
    "                                              replace=False))\n",
    "        \n",
    "        # Copy the parent as the child\n",
    "        child = selection[0]\n",
    "\n",
    "        # Update child vector with choosen parent\n",
    "        for i in range(n_hyperparameters):\n",
    "            child['value'][i] = selection[parent_choice[i]]['value'][i]\n",
    "                    \n",
    "        # Return \n",
    "        return child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034017,
     "end_time": "2021-01-11T11:58:27.562347",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.528330",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From the second generation, we can also use the mutatue function to alter new solutions for the next generation. This function allows us to introduce randomness into the population by randomly altering an aspect of the solution.\n",
    "\n",
    "Mutate Features:\n",
    "1. For each feature in the dataframe, generate a random number between 0 and 1\n",
    "1. If the generated probability is below the user stated mutation rate, then reverse the switch for that column (i.e. if a feature is included then remove it and vice versa).\n",
    "\n",
    "Mutate Hyperparameters:\n",
    "1. For each hyperparameter in the choosen model, generate a random number between 0 and 1\n",
    "1. If the random number is below the user stated mutation rate, the generate a random number between a stated range\n",
    "1. Finally check if the hyperparameter is outside of the min-max range and reduce it to that range if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:27.643796Z",
     "iopub.status.busy": "2021-01-11T11:58:27.643037Z",
     "iopub.status.idle": "2021-01-11T11:58:27.650987Z",
     "shell.execute_reply": "2021-01-11T11:58:27.651553Z"
    },
    "papermill": {
     "duration": 0.055229,
     "end_time": "2021-01-11T11:58:27.651704",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.596475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mutate function\n",
    "def f_gen_child_mutate(candidate, features_name, p_mutate,\n",
    "                       hyperparams, output_type, \n",
    "                       hyperparams_increment):\n",
    "    '''Mutate 2 parents to create a child'''\n",
    "    \n",
    "    # Mutate Features\n",
    "    if output_type == 'feature':\n",
    "        \n",
    "        # Create an integer list of features\n",
    "        l_features = list(range(0,len(features_name)))\n",
    "        \n",
    "        # Convert feature into boolean vector\n",
    "        candidate = [item in candidate for item in l_features]\n",
    "        \n",
    "        # Conditionally mutate features in chromosome (reverse binary flag)          \n",
    "        candidate_new = []\n",
    "        for item in candidate:\n",
    "            if rnd.rand() <= p_mutate:\n",
    "                candidate_new.append(not item)\n",
    "            else:\n",
    "                candidate_new.append(item)\n",
    "                \n",
    "        # Convert to indicies\n",
    "        candidate_new = [i for i,x in enumerate(candidate_new) if x == True]    \n",
    "    \n",
    "        # Return \n",
    "        return candidate_new\n",
    "    \n",
    "    # Mutate hyperparameters\n",
    "    elif (output_type == 'hyperparams') & (hyperparams != False):\n",
    "        \n",
    "        # Identify size of mutation\n",
    "        v_mutate = (np.random.uniform((1-hyperparams_increment), \n",
    "                                      (1+hyperparams_increment), 1)).item()\n",
    "        \n",
    "        # Identify Min and Max for parameters\n",
    "        l_min =  hyperparams['min_value']\n",
    "        l_max =  hyperparams['max_value']\n",
    "        \n",
    "        # Identify the number of parameters\n",
    "        n_hyperparameters = len(l_min)\n",
    "        \n",
    "        # Probabilistically mutate certain parameters\n",
    "        candidate_new = []       \n",
    "        for i in range(n_hyperparameters):\n",
    "            if rnd.rand() <= p_mutate:   \n",
    "                temp = candidate['value'][i] * v_mutate\n",
    "                if hyperparams['type'][i] == 'int':\n",
    "                    temp = np.int64(round(temp))                \n",
    "                candidate_new.append(temp)\n",
    "            else:\n",
    "                candidate_new.append(candidate['value'][i])\n",
    "        \n",
    "        # Ensure that value is between ranges\n",
    "        for i in range(n_hyperparameters):\n",
    "            if (candidate_new[i] < l_min[i]):\n",
    "                candidate_new[i] = l_min[i]\n",
    "            elif (candidate_new[i] > l_max[i]):    \n",
    "                candidate_new[i] = l_max[i]\n",
    "\n",
    "        # Update values                \n",
    "        candidate['value'] = candidate_new\n",
    "\n",
    "        # return\n",
    "        return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035628,
     "end_time": "2021-01-11T11:58:27.722575",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.686947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This function houses the previous three and calls them in succession when generating a new population. The function 'f_random_candidates' is called in the first generation to create an initial population. The subsequent generations make use of the functions 'f_gen_child_crossover' and 'f_gen_child_mutate' to create a new population of candidate solutions (models).\n",
    "\n",
    "This function takes care of the nesting, ultimately the user can specify how many hyperparameters are generated for each feature selection (candidate model). Larger values of nesting will explore a wider range of hyperpameters for each feature selection. The crossover and mutation applies to each version of the candidate with the best hyperparameter found the candidate feature selection maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:27.824515Z",
     "iopub.status.busy": "2021-01-11T11:58:27.823741Z",
     "iopub.status.idle": "2021-01-11T11:58:27.827159Z",
     "shell.execute_reply": "2021-01-11T11:58:27.826657Z"
    },
    "papermill": {
     "duration": 0.06966,
     "end_time": "2021-01-11T11:58:27.827277",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.757617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to generate a population of candidates\n",
    "def f_generate_population(inital_flag, population, features_name, \n",
    "                          p_crossover, p_mutate,\n",
    "                          hyperparams, hyperparams_increment,\n",
    "                          hyperparams_multiple,\n",
    "                          df=False, generation=0, initalise=False):\n",
    "    '''Generates all candidates in population'''\n",
    "    \n",
    "    # Create initial population\n",
    "    if inital_flag == True:\n",
    "        \n",
    "        # Check if there is an initial solution & reduce\n",
    "        # population by one if there is\n",
    "        if initalise != False:\n",
    "            population = population - 1\n",
    "        \n",
    "        # generate random features\n",
    "        df_pop = pd.DataFrame({'generation':generation,\n",
    "                               'candidate':range(0,population),\n",
    "                               'features':f_random_candidates(features_name,\n",
    "                                                              population,\n",
    "                                                              hyperparams,\n",
    "                                                              output_type = 'feature')})\n",
    "        \n",
    "        # Duplicate rows for population range\n",
    "        df_pop = df_pop.loc[df_pop.index.repeat(hyperparams_multiple)]\n",
    "        \n",
    "        # Generate population\n",
    "        df_pop['hyperparameters'] = \\\n",
    "            f_random_candidates(features_name=features_name,\n",
    "                                population = population * hyperparams_multiple,\n",
    "                                hyperparams=hyperparams,\n",
    "                                output_type = 'hyperparams',\n",
    "                                df_pop=df_pop)\n",
    "    \n",
    "        # If Initial solution then add in\n",
    "        if initalise != False:\n",
    "            df = pd.DataFrame({'generation':generation,\n",
    "                               'candidate':range(population, population + 1),\n",
    "                               'features':[initalise['features']],\n",
    "                               'hyperparameters':[initalise['hyperparameters']]},\n",
    "                              index=[population])\n",
    "            \n",
    "            df_pop = df_pop.append(df)\n",
    "        \n",
    "        \n",
    "        # Reset Index\n",
    "        df_pop.index = range(0, population * hyperparams_multiple)\n",
    "        \n",
    "        # Return\n",
    "        return df_pop\n",
    "    else:\n",
    "        # Distribute the population\n",
    "        population_crossover = round(population * p_crossover)\n",
    "        population_remainder = population-population_crossover\n",
    "        \n",
    "        # ----- Create crossover candidates -----\n",
    "        \n",
    "        # Create crossover populate for feature selection\n",
    "        df_pop = pd.DataFrame({'generation':generation,\n",
    "                               'candidate':range(0,population_crossover)})\n",
    "        df_pop['features'] = [f_gen_child_crossover(df=df,\n",
    "                                                    features_name=features_name,\n",
    "                                                    hyperparams=hyperparams,\n",
    "                                                    output_type = 'feature') \\\n",
    "                              for _ in range(population_crossover)]\n",
    "            \n",
    "        # Duplicate rows for population range\n",
    "        df_pop = df_pop.loc[df_pop.index.repeat(hyperparams_multiple)]\n",
    "            \n",
    "        # Create crossover population for hyperparameters\n",
    "        df_pop['hyperparameters'] = \\\n",
    "            [f_gen_child_crossover(df=df,\n",
    "                                   features_name=features_name,\n",
    "                                   hyperparams=hyperparams,\n",
    "                                   output_type = 'hyperparams') \\\n",
    "             for _ in range(population_crossover * hyperparams_multiple)]\n",
    "           \n",
    "        # Reset Index\n",
    "        df_pop.index = range(0, population_crossover * hyperparams_multiple)        \n",
    "                \n",
    "        # ----- Create Randomly Selected candidates -----\n",
    "        \n",
    "        # Initialise population\n",
    "        df_temp = pd.DataFrame({'generation':generation,\n",
    "                                'candidate':range(population_crossover, \n",
    "                                                  population)})   \n",
    "        # Randomly select candidates\n",
    "        selected_index = \\\n",
    "            df.sample(n=population_remainder,\n",
    "                      replace = False, \n",
    "                      weights=df.probability).candidate.tolist()        \n",
    "        \n",
    "        # Extract hyperparameters\n",
    "        selected_features = df.iloc[selected_index,:].features.tolist()\n",
    "        selected_params = df.iloc[selected_index,:].hyperparameters.tolist()\n",
    "        \n",
    "        # Update temp dataframe\n",
    "        df_temp['features'] = [selected_features[i] \n",
    "                               for i in range(len(selected_features))]\n",
    "        df_temp['hyperparameters'] = [selected_params[i] \n",
    "                               for i in range(len(selected_params))]        \n",
    "        \n",
    "        # Duplicate rows for population range\n",
    "        df_temp = df_temp.loc[df_temp.index.repeat(population_remainder)]\n",
    "        \n",
    "        # Append to population dataframe\n",
    "        df_pop = df_pop.append(df_temp,ignore_index=True)\n",
    "        \n",
    "        # Clear up\n",
    "        del selected_features, selected_params, df_temp\n",
    "        \n",
    "        # ----- Mutate Population -----\n",
    "        \n",
    "        # Mutate existing candidate features\n",
    "        df_pop['features'] = \\\n",
    "            df_pop.features.apply(f_gen_child_mutate, \n",
    "                                  features_name=features_name,\n",
    "                                  p_mutate=p_mutate,\n",
    "                                  hyperparams=hyperparams,\n",
    "                                  output_type = 'feature',\n",
    "                                  hyperparams_increment=hyperparams_increment)\n",
    "        \n",
    "        # Mutate existing candidate hyperparameters\n",
    "        df_pop['hyperparameters'] = \\\n",
    "            df_pop.hyperparameters.apply(f_gen_child_mutate,\n",
    "                                         features_name=features_name,\n",
    "                                         p_mutate=p_mutate,\n",
    "                                         hyperparams=hyperparams,\n",
    "                                         output_type = 'hyperparams',\n",
    "                                         hyperparams_increment=\n",
    "                                             hyperparams_increment)      \n",
    "        \n",
    "        # ----- Hyperparameter fix -----\n",
    "        if hyperparams != False:\n",
    "        \n",
    "            # Get length of features\n",
    "            n_features = df_pop['features'].apply(len).tolist()\n",
    "            \n",
    "            # Hyperparameter fix            \n",
    "            for i in range(population):\n",
    "                for j in range(len(hyperparams['names'])):\n",
    "                    if hyperparams['names'][j] == 'max_features':\n",
    "                        if df_pop.hyperparameters[i]['value'][j] > n_features[i] :\n",
    "                            df_pop.hyperparameters[i]['value'][j] = \\\n",
    "                                n_features[i]\n",
    "\n",
    "        # Return\n",
    "        return df_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034482,
     "end_time": "2021-01-11T11:58:27.896425",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.861943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-six-subsection-two\"></a>\n",
    "### Solution Evaluation\n",
    "\n",
    "This section is focussed on evaluating the solutions (models) created in each generation. This includes conducting cross validation and calculating the average AUC across folds.\n",
    "\n",
    "The function below is used to conduct cross validation and get an estimate of the model performance for a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:27.978898Z",
     "iopub.status.busy": "2021-01-11T11:58:27.978204Z",
     "iopub.status.idle": "2021-01-11T11:58:27.980479Z",
     "shell.execute_reply": "2021-01-11T11:58:27.981167Z"
    },
    "papermill": {
     "duration": 0.048675,
     "end_time": "2021-01-11T11:58:27.981313",
     "exception": false,
     "start_time": "2021-01-11T11:58:27.932638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate solution fitness\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def f_fitness(model, eval_metric, features, target, \n",
    "              feature_idx, kfold, hyperparams):\n",
    "    '''Evaluates fitness of proposed solution'''\n",
    "        \n",
    "    # Extract the hyperparameters\n",
    "    n_hyperparams = len(hyperparams['name'])\n",
    "    hyperparameters = {hyperparams['name'][0]:hyperparams['value'][0]}\n",
    "    if n_hyperparams > 1:\n",
    "        for i in range(n_hyperparams):\n",
    "            tempparameters = {hyperparams['name'][i]:hyperparams['value'][i]}\n",
    "            hyperparameters = {**hyperparameters, **tempparameters}\n",
    "    \n",
    "    # Determine CV strategy\n",
    "    if kfold == False:\n",
    "        kfold = 5\n",
    "    else:\n",
    "        kfold = kfold\n",
    "    \n",
    "    # Apply cross validation to the modells\n",
    "    results = cross_val_score(model.set_params(**hyperparameters), \n",
    "                              features.iloc[:,feature_idx], \n",
    "                              target,\n",
    "                              cv=kfold,\n",
    "                              scoring=eval_metric)\n",
    "    \n",
    "    # Replace NA's with 0\n",
    "    results[np.isnan(results)] = 0\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035263,
     "end_time": "2021-01-11T11:58:28.053481",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.018218",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The function below calls the above function to get the performance value for each model. The evaulation for each fold is then averaged to have a final score for that candidate solution (model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:28.127768Z",
     "iopub.status.busy": "2021-01-11T11:58:28.127130Z",
     "iopub.status.idle": "2021-01-11T11:58:28.135021Z",
     "shell.execute_reply": "2021-01-11T11:58:28.135577Z"
    },
    "papermill": {
     "duration": 0.047047,
     "end_time": "2021-01-11T11:58:28.135724",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.088677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply evaluation score to current population\n",
    "def f_evaluation_score(df, features, target, eval_metric, model,\n",
    "                       kfold, hyperparams):\n",
    "    '''Apply f_fitness to each candidate'''\n",
    "    \n",
    "    # Calculate the evaluation metric\n",
    "    evaluation_score = []\n",
    "    for val in range(0, len(df)):\n",
    "        eval_score = f_fitness(model=model,\n",
    "                               eval_metric=eval_metric,\n",
    "                               features = features,\n",
    "                               target=target,\n",
    "                               feature_idx=df['features'][val],\n",
    "                               kfold=kfold,\n",
    "                               hyperparams=df['hyperparameters'][val])\n",
    "        \n",
    "        # Average evaluation metric across folds\n",
    "        evaluation_score.append(eval_score.mean())\n",
    "        \n",
    "        # Clear object\n",
    "        del eval_score\n",
    "    \n",
    "    # Clear object\n",
    "    del val\n",
    "    \n",
    "    # return evaluation score\n",
    "    return evaluation_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035377,
     "end_time": "2021-01-11T11:58:28.206076",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.170699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following three functions are used to calculate the similarity between each solution and the best solution based on the performance metric in a generation. This enables us to track how similar the generated solutions are becoming over time. Jaccard similarity is used for the feature selection element of the algorithm and cosine similarity is used for the hyperparameter similarity.\n",
    "\n",
    "The third function calls the jaccard and cosine functions for each solution to get an estimate of their similarity. It also creates a probability field which is very important for the population generation element of the algorithm. This probability is based on the fitness function and is used as the weighting function when selecting parent solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:28.290109Z",
     "iopub.status.busy": "2021-01-11T11:58:28.289416Z",
     "iopub.status.idle": "2021-01-11T11:58:28.292716Z",
     "shell.execute_reply": "2021-01-11T11:58:28.292169Z"
    },
    "papermill": {
     "duration": 0.051163,
     "end_time": "2021-01-11T11:58:28.292900",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.241737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate jaccard similarity\n",
    "def f_j_sim(list1, list2):\n",
    "    s1 = set(list1)\n",
    "    s2 = set(list2)\n",
    "    return float(len(s1.intersection(s2)) / len(s1.union(s2)))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def f_c_sim(l_other, l_best_score):\n",
    "    \n",
    "    # Extract hyperparameter values\n",
    "    l_other = l_other['value']\n",
    "    \n",
    "    # calculate similarity        \n",
    "    sim = 1 - spatial.distance.cosine(l_best_score, l_other)\n",
    "    \n",
    "    # return\n",
    "    return sim\n",
    "\n",
    "# Calculate similarity between candidates and probability for next gen selection\n",
    "def f_sim_n_prob(df):\n",
    "        \n",
    "    # Calculate similarity of solutions with best solutions - Features\n",
    "    l_best_score = df.features[df['fitness_score'].idxmax()]\n",
    "    df['similarity_features'] = df['features'].apply(f_j_sim, list2=l_best_score)\n",
    "    del l_best_score\n",
    "\n",
    "    # Calculate similarity of solutions with best solutions\n",
    "    l_best_score = df.hyperparameters[df['fitness_score'].idxmax()]['value']\n",
    "    df['similarity_hyperparameters'] = df.hyperparameters.apply(f_c_sim, l_best_score=l_best_score)\n",
    "    del l_best_score \n",
    "        \n",
    "    # Calculate cumulative probability for future stages\n",
    "    df['probability'] = (df['fitness_score'] / sum(df['fitness_score']))\n",
    "    \n",
    "    # return\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03603,
     "end_time": "2021-01-11T11:58:28.364843",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.328813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This function adds the model performance data to storage as well as calculates the number of features included in the model. The function calls the previous solution evaluation functions in turn. One vital bit of functionality is that it allows the user to alter the fitness function from being purely focussed on the evaluation metric (AUC) to including the number of features as part of the modelling process. This ultimately allows the user to determine how important finding a simple model is compared to optimising purely for AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:28.450648Z",
     "iopub.status.busy": "2021-01-11T11:58:28.449917Z",
     "iopub.status.idle": "2021-01-11T11:58:28.452575Z",
     "shell.execute_reply": "2021-01-11T11:58:28.453089Z"
    },
    "papermill": {
     "duration": 0.053032,
     "end_time": "2021-01-11T11:58:28.453236",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.400204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to populate attributes of candidates\n",
    "def f_population_features(df, features, target, desiriability,\n",
    "                          eval_metric, model, kfold, hyperparams):\n",
    "    '''Get features of all candidates in population'''\n",
    "    \n",
    "    # Calculate feature size for candidates\n",
    "    df['feature_size'] = df['features'].apply(len)\n",
    "    \n",
    "    # Calculate evaluation score for candidates\n",
    "    df['evaluation_score'] = f_evaluation_score(df, \n",
    "                                                features, \n",
    "                                                target, \n",
    "                                                eval_metric, \n",
    "                                                model,\n",
    "                                                kfold,\n",
    "                                                hyperparams)\n",
    "    \n",
    "    # Conditionally create desirability fitness score\n",
    "    if desiriability != False:\n",
    "        \n",
    "        # Create scalars - Features\n",
    "        v_lb_features = desiriability['lb'][1]\n",
    "        v_ub_features = desiriability['ub'][1]\n",
    "        v_s_features = desiriability['s'][1]\n",
    "        \n",
    "         # Create scalars - Evaluation Metric\n",
    "        v_lb_eval = desiriability['lb'][0]\n",
    "        v_ub_eval = desiriability['ub'][0]\n",
    "        v_s_eval = desiriability['s'][0]        \n",
    "        \n",
    "        # Calculate desirability for features\n",
    "        df['desire_features']  = [0 if x > v_ub_features else 1 \n",
    "                                  if x < v_lb_features else \n",
    "                                      ((x-v_ub_features)/\n",
    "                                       (v_lb_features-v_ub_features))**\n",
    "                                      v_s_features \n",
    "                                  for x in df['feature_size']]\n",
    "    \n",
    "        # Calculate desirability for evaluation metric\n",
    "        df['desire_eval']  = [0 if x < v_lb_eval else 1\n",
    "                              if x > v_ub_eval else \n",
    "                                      ((x-v_lb_eval)/\n",
    "                                       (v_ub_eval-v_lb_eval))**\n",
    "                                      v_s_eval \n",
    "                              for x in df['evaluation_score']]\n",
    "        \n",
    "        # calculate fitness score\n",
    "        df['fitness_score'] = (df['desire_features'] * df['desire_eval'])**0.5\n",
    "        \n",
    "        # Drop fields\n",
    "        df = df.drop(columns=['desire_features', 'desire_eval'])\n",
    "\n",
    "    else:        \n",
    "        # calculate fitness score\n",
    "        df['fitness_score'] = df['evaluation_score']\n",
    "        \n",
    "    # Return \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035879,
     "end_time": "2021-01-11T11:58:28.524908",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.489029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The final function below is the wrapper function which controls the optimisation process in it's entiriety, it allows the user to state the parameters of the search including:\n",
    "\n",
    "1. Evaluation metrics (any metric accepted by sklearn)\n",
    "1. Which model to use e.g. Elasticnet\n",
    "1. Which hyperparameters are associated with the model (set to false if model doesn't require hyperparameters)\n",
    "1. Desirability (Do we want to optimise purely for the performance metric or do we want to induce model simplicity)\n",
    "1. Cross over rate\n",
    "1. Mutation rate\n",
    "1. Elitism\n",
    "1. Maximum generations without improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:28.599816Z",
     "iopub.status.busy": "2021-01-11T11:58:28.599025Z",
     "iopub.status.idle": "2021-01-11T11:58:28.629580Z",
     "shell.execute_reply": "2021-01-11T11:58:28.630164Z"
    },
    "papermill": {
     "duration": 0.069791,
     "end_time": "2021-01-11T11:58:28.630316",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.560525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main Optimisation Function\n",
    "def f_model_optimisation(df,\n",
    "                         target_var,\n",
    "                         generations, \n",
    "                         population,                   \n",
    "                         eval_metric,\n",
    "                         model,\n",
    "                         kfold=False,\n",
    "                         hyperparams_multiple = 3,\n",
    "                         hyperparams = False,\n",
    "                         desiriability=False,\n",
    "                         p_crossover=0.8,\n",
    "                         p_mutate=0.01,\n",
    "                         hyperparams_increment = 0.1,\n",
    "                         elitism=False,\n",
    "                         gens_no_improve = False,\n",
    "                         initalise = False):\n",
    "    '''Function uses GA's to choose features and tune hyperparameters'''\n",
    "    \n",
    "    # Print Model Stats\n",
    "    print('Model Initialisation')\n",
    "    \n",
    "    # --------- Split features and target ---------\n",
    "    features = df.drop(target_var,axis=1)\n",
    "    features_name = features.columns\n",
    "    target = df[target_var]\n",
    "    \n",
    "    # --------- First Generation ---------\n",
    "    \n",
    "    # Generate inital candidate features solutions\n",
    "    df_pop_cur = f_generate_population(inital_flag=True, \n",
    "                                       population=population, \n",
    "                                       features_name=features_name,\n",
    "                                       p_crossover=p_crossover,\n",
    "                                       p_mutate=p_mutate,\n",
    "                                       hyperparams=hyperparams,\n",
    "                                       hyperparams_increment=hyperparams_increment,\n",
    "                                       hyperparams_multiple=hyperparams_multiple,\n",
    "                                       initalise=initalise)\n",
    "    \n",
    "    # Enrich candidate solutions with features\n",
    "    df_pop_cur = f_population_features(df=df_pop_cur, \n",
    "                                       features=features, \n",
    "                                       target=target,\n",
    "                                       desiriability=desiriability,\n",
    "                                       eval_metric=eval_metric,\n",
    "                                       model=model,\n",
    "                                       kfold=kfold,\n",
    "                                       hyperparams=hyperparams\n",
    "                                       )\n",
    "    \n",
    "    # Extract best score for each candidate\n",
    "    df_pop_cur = df_pop_cur.loc[df_pop_cur.reset_index().\\\n",
    "                                groupby(['candidate'])['fitness_score'].\\\n",
    "                                    idxmax()]\n",
    "    \n",
    "    # Enrich candidate solutions with similarity & probability\n",
    "    df_pop_cur = f_sim_n_prob(df_pop_cur)\n",
    "    \n",
    "    # --------- Create search storage ---------\n",
    "    df_output = df_pop_cur.copy()\n",
    "\n",
    "    # Print Model Stats\n",
    "    print('Gen: 00' +\n",
    "          ' - Generation Mean:' + str(round(df_output.fitness_score.mean(), 4)).zfill(4) +\n",
    "          ' - Generation Best:' + str(round(df_output.fitness_score.max(), 4)).zfill(4) +\n",
    "          ' - Global Best:' + str(round(df_output.fitness_score.max(), 4)).zfill(4)\n",
    "          )\n",
    "    \n",
    "    # Track best solution\n",
    "    if gens_no_improve != False:\n",
    "        count = 0\n",
    "        v_best = df_output.fitness_score.max()\n",
    "    \n",
    "    # --------- Run additional generations ---------\n",
    "\n",
    "    # Loop for additional generations\n",
    "    for gen in range(1, generations):\n",
    "                \n",
    "        # --------- Elitism ---------\n",
    "        if elitism > 0:\n",
    "            \n",
    "            # Create a dataframe with elite candidates\n",
    "            df_elite = df_output.nlargest(columns='fitness_score', n=elitism)\n",
    "            df_elite['candidate'] = population - 1\n",
    "            df_elite['generation'] = gen \n",
    "            df_elite = df_elite.drop(columns=['similarity_features', \n",
    "                                              'similarity_hyperparameters', 'probability'])\n",
    "        # --------- New Population ---------\n",
    "        \n",
    "        # Generate next candidate solutions   \n",
    "        df_pop_cur = f_generate_population(inital_flag=False, \n",
    "                                           generation = gen,\n",
    "                                           population=(population-elitism),\n",
    "                                           features_name=features_name,\n",
    "                                           df=df_pop_cur,\n",
    "                                           p_crossover=p_crossover,\n",
    "                                           p_mutate=p_mutate,\n",
    "                                           hyperparams=hyperparams,\n",
    "                                           hyperparams_increment=hyperparams_increment,\n",
    "                                           hyperparams_multiple=hyperparams_multiple\n",
    "                                           )\n",
    "        \n",
    "        # Enrich candidate solutions with features\n",
    "        df_pop_cur = f_population_features(df=df_pop_cur, \n",
    "                                           features=features, \n",
    "                                           target=target,\n",
    "                                           desiriability=desiriability,\n",
    "                                           eval_metric=eval_metric,\n",
    "                                           model=model,\n",
    "                                           kfold=kfold,\n",
    "                                           hyperparams=hyperparams)\n",
    "        \n",
    "        # Add elite\n",
    "        if elitism > 0:\n",
    "            df_pop_cur = pd.concat([df_pop_cur, df_elite]).reset_index().drop(columns=['index'])\n",
    "            del df_elite\n",
    "               \n",
    "        # Extract best score for each candidate\n",
    "        df_pop_cur = df_pop_cur.loc[df_pop_cur.reset_index().\\\n",
    "                                    groupby(['candidate'])['fitness_score'].\\\n",
    "                                    idxmax()]        \n",
    "        \n",
    "        # Enrich candidate solutions with similarity & probability\n",
    "        df_pop_cur = f_sim_n_prob(df=df_pop_cur)\n",
    "        \n",
    "        # Update Output\n",
    "        df_output = df_output.append(df_pop_cur,ignore_index=True)\n",
    "                \n",
    "        # Print Model Stats\n",
    "        print('Gen: ' + str(gen).zfill(2) +\n",
    "              ' - Generation Mean:' + str(round(df_output[df_output.generation == gen].fitness_score.mean(), 4)).zfill(4) +\n",
    "              ' - Generation Best:' + str(round(df_output[df_output.generation == gen].fitness_score.max(), 4)).zfill(4) +\n",
    "              ' - Global Best:' + str(round(df_output.fitness_score.max(), 4)).zfill(4)\n",
    "              )\n",
    "        \n",
    "        # Track number of generations with no improvement\n",
    "        if gens_no_improve != False:\n",
    "            if df_output.fitness_score.max() > v_best:\n",
    "                count = 0\n",
    "                v_best = df_output.fitness_score.max()\n",
    "            else:\n",
    "                count += 1\n",
    "\n",
    "            # Conditionally break loop\n",
    "            if count == gens_no_improve:\n",
    "                break\n",
    "    \n",
    "    # Return df\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036713,
     "end_time": "2021-01-11T11:58:28.703361",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.666648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-seven\"></a>\n",
    "## Model Optimisation\n",
    "\n",
    "Now that our function is set up we can apply it to our data to see how successful this search will be. In this section we will apply the following models to our optimisaton process.\n",
    "\n",
    "Models:\n",
    "1. ElasticNet\n",
    "1. XGBOOST\n",
    "\n",
    "Within each model there will be a look at the two objectives of maximising AUC as well as inducing sparcity with models.\n",
    "\n",
    "The research surrounding GA suggests that a good setting for the search is:\n",
    "\n",
    "1. Crossover rate = 80%\n",
    "1. Mutation rate = 1 or 2% (2% has been choosen)\n",
    "\n",
    "In the book [Feature Enginerring and Selection: A Practical Approach for Predictive Models](https://bookdown.org/max/FES/) the authors suggest that we should not employ elitism in our optimisation strategy. This ultimately means that the best solution isn't perfectly reserved for the next generation. The ethos here is to avoid getting stuck in a local maxima.\n",
    "\n",
    "Beyond these default values there are a number of other settings which will be tweaked for the individual models based on time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035294,
     "end_time": "2021-01-11T11:58:28.774646",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.739352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-seven-subsection-one\"></a>\n",
    "### ElasticNet\n",
    "\n",
    "ElasticNet allows us to apply regulisation to a regression model and has two hyperparameters which we can tune:\n",
    "\n",
    "* Alpha\n",
    "* L1 ratio\n",
    "\n",
    "In the following subsections the optimisation will be run to optimise purely AUC and then to build a balanced model (good predictive power but as simple as possible).\n",
    "\n",
    "#### Optimise AUC\n",
    "\n",
    "The output below shows the AUC found by the GA, it was run for 5 generations with a population of 20 candidate models with 5 hyperparameters being produced for each cadidate solution. The hyperparameters are banded between a range of 0 & 0.01 for alpha and 0 and 1 for l1_ratio.\n",
    "\n",
    "The output below shows the performance of the model of the generations, we can see that it generally achieves AUC scores in the high 70's. \n",
    "\n",
    "*Note as I have not built a seed into the optimisation model it will produce slightly different results each time when run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T11:58:28.849902Z",
     "iopub.status.busy": "2021-01-11T11:58:28.849164Z",
     "iopub.status.idle": "2021-01-11T12:19:50.475719Z",
     "shell.execute_reply": "2021-01-11T12:19:50.476297Z"
    },
    "papermill": {
     "duration": 1281.666377,
     "end_time": "2021-01-11T12:19:50.476465",
     "exception": false,
     "start_time": "2021-01-11T11:58:28.810088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Optimisation - Optimise for AUC\n",
    "df_ENet_AUC = f_model_optimisation(df=df_train,\n",
    "                                   target_var='TARGET',\n",
    "                                   generations=7, \n",
    "                                   population=20,\n",
    "                                   p_crossover=0.8,\n",
    "                                   p_mutate=0.02,\n",
    "                                   hyperparams_increment=0.01,\n",
    "                                   hyperparams_multiple = 5,\n",
    "                                   eval_metric='roc_auc',\n",
    "                                   kfold=False,\n",
    "                                   model=ElasticNet(),\n",
    "                                   hyperparams = {'names':['alpha', 'l1_ratio'],\n",
    "                                                  'min_value': [0, 0],\n",
    "                                                  'max_value': [0.01, 1],\n",
    "                                                  'type':['float', 'float']}\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038578,
     "end_time": "2021-01-11T12:19:50.553907",
     "exception": false,
     "start_time": "2021-01-11T12:19:50.515329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-seven-subsection-two\"></a>\n",
    "### XGBOOST\n",
    "\n",
    "The final model which we will run through our optimisation is XGBOOST, again this model has multiple hyperparameters which can be tuned, the one's which will be focussed on are:\n",
    "\n",
    "1. learning_rate = Control the weighting of new trees added to the model\n",
    "1. max_depth = The maximum depth of a tree\n",
    "1. min_child_weight = The minimum sum of weights of all observations required in a child\n",
    "1. gamma = minimum loss reduction required to make a split\n",
    "1. colsample_bytree = The fraction of columns to be randomly samples for each tree\n",
    "\n",
    "#### Optimise AUC\n",
    "\n",
    "The GA search was run for 5 generations with a population of 20 candidate models with 5 hyperparameter variants being produced for each cadidate solution. The hyperparameter ranges are:\n",
    "\n",
    "* learning_rate = [0.03, 0.3]\n",
    "* max_depth = [2, 15]\n",
    "* min_child_weight = [1, 7]\n",
    "* gamma = [0, 0.5]\n",
    "* colsample_bytree = [0.3, 0.7]\n",
    "\n",
    "The output below shows the performance of the model across the generations, we can see that it generally achieves AUC scores in the low-mid 80's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T12:19:50.708843Z",
     "iopub.status.busy": "2021-01-11T12:19:50.706000Z",
     "iopub.status.idle": "2021-01-11T18:37:43.090807Z",
     "shell.execute_reply": "2021-01-11T18:37:43.092092Z"
    },
    "papermill": {
     "duration": 22672.498223,
     "end_time": "2021-01-11T18:37:43.092475",
     "exception": false,
     "start_time": "2021-01-11T12:19:50.594252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run Optimisation - Optimise for AUC\n",
    "df_xgb_AUC = f_model_optimisation(df=df_train,\n",
    "                                  target_var='TARGET',\n",
    "                                  generations=5, \n",
    "                                  population=20,\n",
    "                                  p_crossover=0.8,\n",
    "                                  p_mutate=0.02,\n",
    "                                  hyperparams_increment=0.01,\n",
    "                                  hyperparams_multiple = 5,\n",
    "                                  eval_metric='roc_auc',\n",
    "                                  kfold=False,\n",
    "                                  model=XGBClassifier(objective=\"binary:logistic\", scale_pos_weight = 25),\n",
    "                                  hyperparams = {'names':['learning_rate', 'max_depth', \n",
    "                                                          'min_child_weight', 'gamma', 'colsample_bytree'],\n",
    "                                                 'min_value': [0.03, 2,  1, 0,   0.3],\n",
    "                                                 'max_value': [0.3,  15, 7, 0.5, 0.7],\n",
    "                                                 'type':['float', 'int', 'int', 'float', 'float']}\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049938,
     "end_time": "2021-01-11T18:37:43.204018",
     "exception": false,
     "start_time": "2021-01-11T18:37:43.154080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-seven-subsection-three\"></a>\n",
    "### Analysis Findings\n",
    "\n",
    "The results for all candidate models assessed by the GA were stored and now can be investigated. The graphic below shows the mean and max AUC achieved by each model across the generations. We can see that the mean scores across the generations are generally increasing indicating that the GA is driving towards more similar solutions across generations. The max score doesn't necessarily increase between generations as it reflects accepting worse solutions to enter new parts of the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T18:37:43.322651Z",
     "iopub.status.busy": "2021-01-11T18:37:43.316779Z",
     "iopub.status.idle": "2021-01-11T18:37:43.846474Z",
     "shell.execute_reply": "2021-01-11T18:37:43.847201Z"
    },
    "papermill": {
     "duration": 0.599484,
     "end_time": "2021-01-11T18:37:43.847361",
     "exception": false,
     "start_time": "2021-01-11T18:37:43.247877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summarise EN scores\n",
    "df_temp_EN = df_ENet_AUC.groupby('generation')['evaluation_score'].agg(['mean', 'max']).reset_index()\n",
    "df_temp_EN['model'] = 'ElasticNet'\n",
    "\n",
    "# Summarise XG scores\n",
    "df_temp_XG = df_xgb_AUC.groupby('generation')['evaluation_score'].agg(['mean', 'max']).reset_index()\n",
    "df_temp_XG['model'] = 'XGBOOST'\n",
    "\n",
    "# Combine Dataframes\n",
    "df_vis = pd.concat([df_temp_EN, df_temp_XG])\n",
    "\n",
    "# Plot evaluation score\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,6))\n",
    "fig.suptitle(\"Average and Best Evaluation Scores Per Generation\", fontsize=16)    \n",
    "ax[0].set_title(\"ELasticNet\")\n",
    "ax[1].set_title(\"XGBOOST\")\n",
    "sns.lineplot(data = df_temp_EN, x= 'generation', y='mean', color=\"blue\", label='Mean', ax = ax[0])\n",
    "sns.lineplot(data = df_temp_EN, x= 'generation', y='max', color=\"red\", label='Max', ax = ax[0])\n",
    "sns.lineplot(data = df_temp_XG, x= 'generation', y='mean', color=\"blue\", label='Mean', ax = ax[1])\n",
    "sns.lineplot(data = df_temp_XG, x= 'generation', y='max', color=\"red\", label='Max', ax = ax[1])\n",
    "ax[0].set(ylim=(0.7, 0.85))\n",
    "ax[1].set(ylim=(0.7, 0.85))\n",
    "ax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.setp(ax[:], xlabel='Generation')\n",
    "plt.setp(ax[:], ylabel='AUC')\n",
    "plt.show()\n",
    "\n",
    "# Clear objects\n",
    "del df_temp_EN, df_temp_XG, fig, df_vis, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.044207,
     "end_time": "2021-01-11T18:37:43.934939",
     "exception": false,
     "start_time": "2021-01-11T18:37:43.890732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above we can see that the mean scores are increasing, the distribution of features are becoming much tighter as the model converges towards popular predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T18:37:44.043191Z",
     "iopub.status.busy": "2021-01-11T18:37:44.037426Z",
     "iopub.status.idle": "2021-01-11T18:37:44.513004Z",
     "shell.execute_reply": "2021-01-11T18:37:44.513464Z"
    },
    "papermill": {
     "duration": 0.535877,
     "end_time": "2021-01-11T18:37:44.513639",
     "exception": false,
     "start_time": "2021-01-11T18:37:43.977762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EN scores\n",
    "df_temp_EN = df_ENet_AUC\n",
    "\n",
    "# XG scores\n",
    "df_temp_XG = df_xgb_AUC\n",
    "\n",
    "# Plot evaluation score\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,6))\n",
    "fig.suptitle(\"Distribution of Feature Size Per Generation\", fontsize=16)    \n",
    "ax[0].set_title(\"ELasticNet\")\n",
    "ax[1].set_title(\"XGBOOST\")\n",
    "sns.boxplot(data = df_temp_EN, x= 'generation', y='feature_size', color=\"blue\", ax = ax[0])\n",
    "sns.boxplot(data = df_temp_XG, x= 'generation', y='feature_size', color=\"red\", ax = ax[1])\n",
    "ax[0].set(ylim=(0, 369))\n",
    "ax[1].set(ylim=(0, 369))\n",
    "plt.setp(ax[:], xlabel='Generation')\n",
    "plt.setp(ax[:], ylabel='Number of Features')\n",
    "plt.show()\n",
    "\n",
    "# Clear objects\n",
    "del df_temp_EN, df_temp_XG, fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043559,
     "end_time": "2021-01-11T18:37:44.602621",
     "exception": false,
     "start_time": "2021-01-11T18:37:44.559062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The chart below shows the average similarity within the population against the best solution found in a generation. Similarity is in reference to the features included in the model using JACCARD similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T18:37:44.713682Z",
     "iopub.status.busy": "2021-01-11T18:37:44.707339Z",
     "iopub.status.idle": "2021-01-11T18:37:45.097691Z",
     "shell.execute_reply": "2021-01-11T18:37:45.097001Z"
    },
    "papermill": {
     "duration": 0.449463,
     "end_time": "2021-01-11T18:37:45.097807",
     "exception": false,
     "start_time": "2021-01-11T18:37:44.648344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summarise EN scores\n",
    "df_temp_EN = df_ENet_AUC.groupby('generation')['similarity_features'].agg(['mean']).reset_index()\n",
    "df_temp_EN['model'] = 'ElasticNet'\n",
    "\n",
    "# Summarise XG scores\n",
    "df_temp_XG = df_xgb_AUC.groupby('generation')['similarity_features'].agg(['mean', 'max']).reset_index()\n",
    "df_temp_XG['model'] = 'XGBOOST'\n",
    "\n",
    "# Combine Dataframes\n",
    "df_vis = pd.concat([df_temp_EN, df_temp_XG])\n",
    "\n",
    "# Plot evaluation score\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,6))\n",
    "fig.suptitle(\"Average Similarity Between Candidate Features and Best Solution's Per Generation\", fontsize=16)    \n",
    "ax[0].set_title(\"ELasticNet\")\n",
    "ax[1].set_title(\"XGBOOST\")\n",
    "sns.lineplot(data = df_temp_EN, x= 'generation', y='mean', color=\"blue\", label='Mean', ax = ax[0])\n",
    "sns.lineplot(data = df_temp_XG, x= 'generation', y='mean', color=\"blue\", label='Mean', ax = ax[1])\n",
    "ax[0].set(ylim=(0, 1))\n",
    "ax[1].set(ylim=(0, 1))\n",
    "ax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.setp(ax[:], xlabel='Generation')\n",
    "plt.setp(ax[:], ylabel='Similarity')\n",
    "plt.show()\n",
    "\n",
    "# Clear objects\n",
    "del df_temp_EN, df_temp_XG, fig, df_vis, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.044086,
     "end_time": "2021-01-11T18:37:45.187937",
     "exception": false,
     "start_time": "2021-01-11T18:37:45.143851",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see that the hyperparameters conver much faster, likely due to the smaller range of values to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T18:37:45.437697Z",
     "iopub.status.busy": "2021-01-11T18:37:45.434050Z",
     "iopub.status.idle": "2021-01-11T18:37:45.776624Z",
     "shell.execute_reply": "2021-01-11T18:37:45.775956Z"
    },
    "papermill": {
     "duration": 0.54366,
     "end_time": "2021-01-11T18:37:45.776742",
     "exception": false,
     "start_time": "2021-01-11T18:37:45.233082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summarise EN scores\n",
    "df_temp_EN = df_ENet_AUC.groupby('generation')['similarity_hyperparameters'].agg(['mean']).reset_index()\n",
    "df_temp_EN['model'] = 'ElasticNet'\n",
    "\n",
    "# Summarise XG scores\n",
    "df_temp_XG = df_xgb_AUC.groupby('generation')['similarity_hyperparameters'].agg(['mean', 'max']).reset_index()\n",
    "df_temp_XG['model'] = 'XGBOOST'\n",
    "\n",
    "# Combine Dataframes\n",
    "df_vis = pd.concat([df_temp_EN, df_temp_XG])\n",
    "\n",
    "# Plot evaluation score\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,6))\n",
    "fig.suptitle(\"Average Similarity Between Candidate Hyperparameters and Best Candidate Per Generation\", fontsize=16)    \n",
    "ax[0].set_title(\"ELasticNet\")\n",
    "ax[1].set_title(\"XGBOOST\")\n",
    "sns.lineplot(data = df_temp_EN, x= 'generation', y='mean', color=\"blue\", ax = ax[0])\n",
    "sns.lineplot(data = df_temp_XG, x= 'generation', y='mean', color=\"blue\", ax = ax[1])\n",
    "ax[0].set(ylim=(0, 1))\n",
    "ax[1].set(ylim=(0, 1))\n",
    "ax[0].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax[1].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.setp(ax[:], xlabel='Generation')\n",
    "plt.setp(ax[:], ylabel='Similarity')\n",
    "plt.show()\n",
    "\n",
    "# Clear objects\n",
    "del df_temp_EN, df_temp_XG, fig, df_vis, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047878,
     "end_time": "2021-01-11T18:37:45.870249",
     "exception": false,
     "start_time": "2021-01-11T18:37:45.822371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-eight\"></a>\n",
    "## Model Evaluation\n",
    "\n",
    "We can now use the best model found by the GA and use this to predict against our validation dataset, the output below shows how the model performs on the training set in CV and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T18:37:45.977462Z",
     "iopub.status.busy": "2021-01-11T18:37:45.972042Z",
     "iopub.status.idle": "2021-01-11T18:38:05.872981Z",
     "shell.execute_reply": "2021-01-11T18:38:05.872218Z"
    },
    "papermill": {
     "duration": 19.956101,
     "end_time": "2021-01-11T18:38:05.873150",
     "exception": false,
     "start_time": "2021-01-11T18:37:45.917049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "# -------------------------------------------------- ElasticNet -------------------------------------------------\n",
    "#################################################################################################################\n",
    "\n",
    "# Extract best solution\n",
    "df_best_en = df_ENet_AUC[df_ENet_AUC.fitness_score == max(df_ENet_AUC.fitness_score)]\n",
    "\n",
    "# Extract features\n",
    "l_best_features_en = (df_best_en.features.tolist())[0]\n",
    "\n",
    "# Extract the hyperparameters\n",
    "l_best_hyperparms_en = (df_best_en.hyperparameters.tolist())[0]\n",
    "n_hyperparams_en = len(l_best_hyperparms_en['name'])\n",
    "hyperparameters_en = {l_best_hyperparms_en['name'][0]:l_best_hyperparms_en['value'][0]}\n",
    "if n_hyperparams_en > 1:\n",
    "    for i in range(n_hyperparams_en):\n",
    "        tempparameters = {l_best_hyperparms_en['name'][i]:l_best_hyperparms_en['value'][i]}\n",
    "        hyperparameters_en = {**hyperparameters_en, **tempparameters}\n",
    "\n",
    "# Create model object\n",
    "model_en = ElasticNet()\n",
    "\n",
    "# Set Hyperparameters\n",
    "model_en.set_params(**hyperparameters_en)\n",
    "\n",
    "# Fit model\n",
    "model_en.fit(df_train.iloc[:,l_best_features_en], df_train.TARGET)\n",
    "\n",
    "# Predict on validation\n",
    "y_pred_en = model_en.predict(df_validation.iloc[:,l_best_features_en])\n",
    "\n",
    "\n",
    "#################################################################################################################\n",
    "# --------------------------------------------------- XGBOOST ---------------------------------------------------\n",
    "#################################################################################################################\n",
    "\n",
    "# Extract best solution\n",
    "df_best_xg = df_xgb_AUC[df_xgb_AUC.fitness_score == max(df_xgb_AUC.fitness_score)]\n",
    "\n",
    "# Extract features\n",
    "l_best_features_xg = (df_best_xg.features.tolist())[0]\n",
    "\n",
    "# Extract the hyperparameters\n",
    "l_best_hyperparms_xg = (df_best_xg.hyperparameters.tolist())[0]\n",
    "n_hyperparams_xg = len(l_best_hyperparms_xg['name'])\n",
    "hyperparameters_xg = {l_best_hyperparms_xg['name'][0]:l_best_hyperparms_xg['value'][0]}\n",
    "if n_hyperparams_xg > 1:\n",
    "    for i in range(n_hyperparams_xg):\n",
    "        tempparameters = {l_best_hyperparms_xg['name'][i]:l_best_hyperparms_xg['value'][i]}\n",
    "        hyperparameters_xg = {**hyperparameters_xg, **tempparameters}\n",
    "\n",
    "# Create model object\n",
    "model_xg = XGBClassifier(scale_pos_weight = 25)\n",
    "\n",
    "# Set Hyperparameters\n",
    "model_xg.set_params(**hyperparameters_xg)\n",
    "\n",
    "# Fit model\n",
    "model_xg.fit(df_train.iloc[:,l_best_features_xg], df_train.TARGET)\n",
    "\n",
    "# Predict on validation\n",
    "y_pred_temp = ((model_xg.predict_proba(df_validation.iloc[:,l_best_features_xg])).tolist())\n",
    "y_pred_xg = []\n",
    "for i in range(len(df_validation)):\n",
    "    y_pred_xg.append(y_pred_temp[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T18:38:06.002672Z",
     "iopub.status.busy": "2021-01-11T18:38:06.001623Z",
     "iopub.status.idle": "2021-01-11T18:38:06.026233Z",
     "shell.execute_reply": "2021-01-11T18:38:06.026930Z"
    },
    "papermill": {
     "duration": 0.094566,
     "end_time": "2021-01-11T18:38:06.027116",
     "exception": false,
     "start_time": "2021-01-11T18:38:05.932550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################################################################\n",
    "# ------------------------------------------------ Output Scores ------------------------------------------------\n",
    "#################################################################################################################\n",
    "\n",
    "# Evaluation scores\n",
    "print('ElasticNET')\n",
    "print('Train Set AUC:', round((df_best_en.evaluation_score.tolist())[0], 3))\n",
    "print('Validation Set AUC:', round(metrics.roc_auc_score(df_validation.TARGET, y_pred_en), 3))\n",
    "print(' ')\n",
    "\n",
    "# Evaluation scores\n",
    "print('XGBOOST')\n",
    "print('Train Set AUC:', round((df_best_xg.evaluation_score.tolist())[0], 3))\n",
    "print('Validation Set AUC:', round(metrics.roc_auc_score(df_validation.TARGET, y_pred_xg), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047997,
     "end_time": "2021-01-11T18:38:06.130290",
     "exception": false,
     "start_time": "2021-01-11T18:38:06.082293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-nine\"></a>\n",
    "## Conclusion\n",
    "\n",
    "To conclude, GA appears to be a useful tool to apply both feature selection and hyperparameter tuning. The main drawback found during this process is the computational commitment required, while my code can 100% be optimised it is a relatively significant time committment. Taking the execution point out of consideration for the moment the GA does typically drive the modelling solution in the right areas of the search space.\n",
    "\n",
    "In practice I might be more tempted to use GA to generate sparse models with reasonable predictive power rather than to maximise a performance metric. I might explore this in another notebook.\n",
    "\n",
    "It has been good practice playing with functions and creating a workflow for both machine learning models and for a hack implementation of a GA for hyperparameter tuning and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049759,
     "end_time": "2021-01-11T18:38:06.228104",
     "exception": false,
     "start_time": "2021-01-11T18:38:06.178345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-ten\"></a>\n",
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-01-11T18:38:06.853884Z",
     "iopub.status.busy": "2021-01-11T18:38:06.845899Z",
     "iopub.status.idle": "2021-01-11T18:38:08.700577Z",
     "shell.execute_reply": "2021-01-11T18:38:08.699887Z"
    },
    "papermill": {
     "duration": 2.425254,
     "end_time": "2021-01-11T18:38:08.700704",
     "exception": false,
     "start_time": "2021-01-11T18:38:06.275450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read and submit score\n",
    "y_pred_temp = ((model_xg.predict_proba(df_test.iloc[:,l_best_features_xg])).tolist())\n",
    "y_pred = []\n",
    "for i in range(len(df_test)):\n",
    "    y_pred.append(y_pred_temp[i][1])\n",
    "submission = pd.read_csv('../input/santander-customer-satisfaction/sample_submission.csv', encoding='latin-1')\n",
    "submission.loc[:, 'TARGET'] = y_pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046355,
     "end_time": "2021-01-11T18:38:08.794176",
     "exception": false,
     "start_time": "2021-01-11T18:38:08.747821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-eight\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "papermill": {
   "duration": 23999.797522,
   "end_time": "2021-01-11T18:38:08.951330",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-11T11:58:09.153808",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
